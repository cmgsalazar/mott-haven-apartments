{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c2c0a2b-f1ac-4980-bdfb-1a2350265d55",
   "metadata": {},
   "source": [
    "### StreetEasy Mott Haven Buildings, Attempt 2\n",
    "\n",
    "Previously, we attempted to scrape the [list of buildings in Mott Haven](https://streeteasy.com/buildings/mott-haven) from the StreetEasy website through `BeautifulSoup`. However, the page seems to use JS to dynamically load content. So, we'll try scraping through headless browser `Playwright`.\n",
    "\n",
    "The details we need are the following:\n",
    "\n",
    "* building name\n",
    "* address\n",
    "* coordinates (for mapping)\n",
    "* year it was built\n",
    "* number of stories\n",
    "* number of units\n",
    "* link to individual pages (which we will use to get more details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407f78fb-3421-4035-8047-3c0f722e3b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "from playwright.sync_api import sync_playwright, Playwright\n",
    "from random import randrange\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f5bb29c-eca3-456b-a300-38b0eca6ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snoozer\n",
    "\n",
    "def snoozer(start_time, end_time):\n",
    "    '''\n",
    "    This function creates a snoozer that can be used when scraping.\n",
    "    It requires `from random import randrange` and `import time`. \n",
    "    \n",
    "    Parameters: \n",
    "    start_time (int) = start time of range, in seconds\n",
    "    end_time (int) = end time of range, in seconds\n",
    "    '''\n",
    "    timer = randrange(start_time, end_time)\n",
    "    print(f\"Snoozing for {timer} seconds...\")\n",
    "    time.sleep(timer)\n",
    "    print(\"\") # adds a line break for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e4451-5ae5-4682-9e78-b1d3cc464348",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAIN CODE\n",
    "# this is tweaked from the bs4 scraper version\n",
    "\n",
    "base_url = \"https://streeteasy.com/buildings/mott-haven\"\n",
    "end_page = 45 # number of pages we want to scrape\n",
    "errors_list = [] # holds pages with errors\n",
    "main_df = [] # holds all captured lists\n",
    "\n",
    "# starting playwright\n",
    "with sync_playwright() as playwright:\n",
    "    broswer = playwright.firefox.launch(headless=False) # False because I want to see it\n",
    "    page = browser.new_page()\n",
    "    run(playwright)\n",
    "\n",
    "for page_num in range(1, end_page + 1):\n",
    "    print(f\"Page {page_num}:\")\n",
    "    \n",
    "    ## requesting URLs\n",
    "    url = f\"{base_url}?page={page_num}\" if page_num != 1 else base_url\n",
    "    page.goto(url)\n",
    "    \n",
    "    if not status_check(response):\n",
    "        errors_list.append(url)\n",
    "        continue\n",
    "    \n",
    "    ## extracting data\n",
    "    # target_items = soup.find_all(\"li\", class_=\"item building\")\n",
    "\n",
    "    # building_names = [ target.find(\"h2\", class_=\"details-title\").get_text(strip=True) for target in target_items ]\n",
    "    # building_links = [ \"https://streeteasy.com\" + target.find(\"a\").get(\"href\") for target in target_items]\n",
    "    # building_addresses = [ target.find(\"ul\".find(\"li\").get_text(strip=True) for target in target_items ]\n",
    "    # building_latlng = [ target.get(\"se:map:point\") for target in target_items]\n",
    "\n",
    "    # ## other data held separately\n",
    "\n",
    "    # for target in target_items:        \n",
    "    #     other_details = target.find(\"ul\", class_=\"details_info\").find_all(\"li\", class_=\"detail_cell\")        \n",
    "    #     building_units = [ other_details[0].replace(\" units\", \"\") ]\n",
    "    #     building_stories = [ other_details[1].replace(\" stories\", \"\") ]\n",
    "    #     building_year = [ other_details[2].replace(\"built in\", \"\") ]\n",
    "\n",
    "    # ## create df to hold all data\n",
    "\n",
    "    # main_df.append(pd.DataFrame({ \"building_name\": building_names,\n",
    "    #                              \"link\": building_links,\n",
    "    #                              \"address\": building_addresses,\n",
    "    #                              \"coordinates\": building_latlng,\n",
    "    #                              \"year_built\": building_year,\n",
    "    #                              \"total_stories\": building_stories,\n",
    "    #                              \"total_units\": building_units\n",
    "    #                             }))\n",
    "\n",
    "    snoozer(21, 56)\n",
    "\n",
    "# closing playwright\n",
    "browser.close()\n",
    "print(f\"Done scraping {end_page} pages!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afa2101-b394-4904-8234-a085655782d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
