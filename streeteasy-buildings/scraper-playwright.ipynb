{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c2c0a2b-f1ac-4980-bdfb-1a2350265d55",
   "metadata": {},
   "source": [
    "### StreetEasy Mott Haven Buildings, Attempt 2\n",
    "\n",
    "Previously, we attempted to scrape the [list of buildings in Mott Haven](https://streeteasy.com/buildings/mott-haven) from the StreetEasy website through `BeautifulSoup`. However, the page seems to use JS to dynamically load content. So, we'll try scraping through headless browser `Playwright`.\n",
    "\n",
    "The details we need are the following:\n",
    "\n",
    "* building name\n",
    "* address\n",
    "* coordinates (for mapping)\n",
    "* year it was built\n",
    "* number of stories\n",
    "* number of units\n",
    "* link to individual pages (which we will use to get more details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "407f78fb-3421-4035-8047-3c0f722e3b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "from playwright.async_api import async_playwright\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randrange\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f5bb29c-eca3-456b-a300-38b0eca6ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snoozer\n",
    "\n",
    "def snoozer(start_time, end_time):\n",
    "    '''\n",
    "    This function creates a snoozer that can be used when scraping.\n",
    "    It requires `from random import randrange` and `import time`. \n",
    "    \n",
    "    Parameters: \n",
    "    start_time (int) = start time of range, in seconds\n",
    "    end_time (int) = end time of range, in seconds\n",
    "    '''\n",
    "    timer = randrange(start_time, end_time)\n",
    "    print(f\"Snoozing for {timer} seconds...\")\n",
    "    time.sleep(timer)\n",
    "    print(\"\") # adds a line break for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bac985-7f62-4b9e-b507-123d7e9c50ce",
   "metadata": {},
   "source": [
    "### Apparently, it's more convenient to `soup`-ify Playwright `page.content()`\n",
    "\n",
    "This worked... until I was denied access to the webpage. (I used VPN!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e16896-53e6-41df-92d7-1eb60eff5583",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SOMA VERSION\n",
    "\n",
    "base_url = \"https://streeteasy.com/buildings/mott-haven\"\n",
    "end_page = 1 # number of pages we want to scrape\n",
    "### for now, `end_page` is set to 1... but when this code works, it should be changed back to 45.\n",
    "errors_list = [] # holds pages with errors\n",
    "all_data = [] # holds all captured lists\n",
    "    \n",
    "for page_num in range(1, end_page + 1):\n",
    "    # starting playwright\n",
    "    playwright = await async_playwright().start()\n",
    "    browser = await playwright.firefox.launch(headless=False) # False because I want to see it load the page\n",
    "    page = await browser.new_page()\n",
    "    \n",
    "    print(f\"Attempting to scrape page {page_num}...\")\n",
    "        \n",
    "    try:\n",
    "        ## requesting URLs\n",
    "        if page_num != 1:\n",
    "            url = f\"{base_url}?page={page_num}\"\n",
    "        else:\n",
    "            url = base_url\n",
    "        await page.goto(url)\n",
    "        soup = BeautifulSoup(await page.content()) # Playwright -> BeautifulSoup is easier?\n",
    "    \n",
    "        await playwright.stop() # closes the browser? in a way it skips the access check-in parts\n",
    "        \n",
    "        # extracting data\n",
    "        target_items = soup.find_all(\"li\", class_=\"item building\")\n",
    "        \n",
    "        building_names = [ target.find(\"h2\", class_=\"details-title\").get_text(strip=True).replace(\"SAVE\", \"\") for target in target_items ]\n",
    "        building_links = [ \"https://streeteasy.com\" + target.find(\"a\").get(\"href\") for target in target_items]\n",
    "        building_addresses = [ target.find(\"ul\").find(\"li\").replace(\"At \", \"\").get_text(strip=True) if \"At \" in target.find(\"ul\").find(\"li\").get_text()\\\n",
    "                              else np.nan for target in target_items ]\n",
    "        building_latlng = [ target.get(\"se:map:point\") for target in target_items]\n",
    "    \n",
    "        ## other data held separately\n",
    "\n",
    "        # initializing lists, so they can exist outside the for loop below\n",
    "        building_units = []\n",
    "        building_stories = [] \n",
    "        building_year = []\n",
    "\n",
    "        for target in target_items:\n",
    "            other_details = target.find(\"ul\", class_=\"details_info\")\n",
    "            if other_details: # this temporarily places NaN, will be replaced if `detail` is found\n",
    "                units = np.nan\n",
    "                stories = np.nan\n",
    "                year = np.nan\n",
    "            for detail in other_details.find_all(\"li\", class_=\"detail_cell\"):\n",
    "                text = detail.text.strip()\n",
    "                if \"units\" in text:\n",
    "                    units = int(text.split()[0]) \n",
    "                elif \"stories\" in text:\n",
    "                    stories = int(text.split()[0]) \n",
    "                elif \"built in\" in text:\n",
    "                    year = int(text.split()[-1]) \n",
    "            # appending lists        \n",
    "            building_units.append(units)\n",
    "            building_stories.append(stories)\n",
    "            building_year.append(year)\n",
    "        \n",
    "        print(f\"Successfuly scraped page {page_num}!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        errors_list.append(url)\n",
    "        print(f\"Error '{e}' was found on {url}, page {page_num} of {end_page} pages. Moving to next scrape...\")\n",
    "    \n",
    "    finally:\n",
    "        # checking to see if they all have the same range\n",
    "        print(len(building_names))\n",
    "        print(len(building_links))\n",
    "        print(len(building_addresses))\n",
    "        print(len(building_latlng))\n",
    "        print(len(building_year))\n",
    "        print(len(building_stories))\n",
    "        print(len(building_units))\n",
    "\n",
    "        # create df to hold all data\n",
    "        all_data.append(pd.DataFrame({ \"building_name\": building_names,\n",
    "                                     \"link\": building_links,\n",
    "                                     \"address\": building_addresses,\n",
    "                                     \"coordinates\": building_latlng,\n",
    "                                     \"year_built\": building_year,\n",
    "                                     \"total_stories\": building_stories,\n",
    "                                     \"total_units\": building_units\n",
    "                                    }))\n",
    "        \n",
    "        if page_num <= end_page - 1:\n",
    "            snoozer(21, 56)\n",
    "    \n",
    "print(f\"Done scraping {page_num} of {end_page} pages!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafd1478-0b30-46c8-b23d-829ee8176d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting this to our final df\n",
    "\n",
    "final_df = pd.concat(all_data, ignore_index=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "23949956-2140-4959-a9e7-3b226fed65c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the file to csv\n",
    "\n",
    "final_df.to_csv(\"mott-haven-streeteasy-buildings.csv\", encoding=\"UTF-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0ba58b-2264-4b44-91e6-46bf3c6b2490",
   "metadata": {},
   "source": [
    "### This was the original Playwright code I've been working on... \n",
    "\n",
    "A recurring problem is access check-ins then denials when we try to jump to the next page. I haven't run the scrapers yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e4451-5ae5-4682-9e78-b1d3cc464348",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAIN CODE\n",
    "# this is tweaked from the bs4 scraper version\n",
    "\n",
    "base_url = \"https://streeteasy.com/buildings/mott-haven\"\n",
    "end_page = 2 # number of pages we want to scrape\n",
    "### for now, `end_page` is set to 2... but when this code works, it should be changed back to 45.\n",
    "errors_list = [] # holds pages with errors\n",
    "main_df = [] # holds all captured lists\n",
    "\n",
    "# starting playwright\n",
    "async def scraper():\n",
    "    '''\n",
    "    This function scrapes a page asynchronously.\n",
    "    '''\n",
    "    async with async_playwright() as playwright:\n",
    "        browser = await playwright.firefox.launch(headless=False) # False because I want to see it load the page\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "    \n",
    "        try:\n",
    "            for page_num in range(1, end_page + 1):\n",
    "                print(f\"Attempting to scrape page {page_num}...\")\n",
    "                \n",
    "                ## requesting URLs\n",
    "                if page_num != 1:\n",
    "                    url = f\"{base_url}?page={page_num}\"\n",
    "                else:\n",
    "                    url = base_url\n",
    "                await page.goto(url)\n",
    "                await page.wait_for_load_state('networkidle')\n",
    "        \n",
    "                ## extracting data\n",
    "                # target_items = await page.query_selector_all(\"li.item.building\")\n",
    "            \n",
    "                # building_names = [ await target.query_selector(\"h2.details-title\").inner_text() for target in target_items ]\n",
    "                # building_links = [ \"https://streeteasy.com\" + await query_selector(\"a\").get_attribute(\"href\") for target in target_items]\n",
    "                # building_addresses = [ await target.query_selector(\"ul li\").inner_text() for target in target_items ]\n",
    "                # building_latlng = [ await target.get_attribute(\"se:map:point\") for target in target_items]\n",
    "            \n",
    "                # ## other data held separately\n",
    "                # for target in target_items:        \n",
    "                #     other_details = await target.query_selector(\"ul.details_info li.detail_cell\")        \n",
    "                #     building_units = [ await other_details[0].inner_text().replace(\" units\", \"\").strip() ]\n",
    "                #     building_stories = [ await other_details[1].inner_text().replace(\" stories\", \"\").strip() ]\n",
    "                #     building_year = [ await other_details[2].inner_text().replace(\"built in\", \"\").strip() ]\n",
    "    \n",
    "                print(f\"Successfuly scraped page {page_num}!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            errors_list.append(url)\n",
    "            print(f\"Error '{e}' was found on {url}, page {page_num} of {end_page} pages. Moving to next scrape...\")\n",
    "            \n",
    "        finally:\n",
    "            # ## create df to hold all data\n",
    "            # main_df.append(pd.DataFrame({ \"building_name\": building_names,\n",
    "                                         # \"link\": building_links,\n",
    "                                         # \"address\": building_addresses,\n",
    "                                         # \"coordinates\": building_latlng,\n",
    "                                         # \"year_built\": building_year,\n",
    "                                         # \"total_stories\": building_stories,\n",
    "                                         # \"total_units\": building_units\n",
    "                                        # }))\n",
    "            \n",
    "            snoozer(21, 56)\n",
    "    \n",
    "            # closing playwright\n",
    "            await browser.close()\n",
    "            print(f\"Done scraping {end_page} pages!\") \n",
    "\n",
    "nest_asyncio.apply()\n",
    "asyncio.run(scraper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5006f244-666a-49e4-be02-ec3fb7f158c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
