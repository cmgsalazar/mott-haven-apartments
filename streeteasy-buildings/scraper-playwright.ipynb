{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c2c0a2b-f1ac-4980-bdfb-1a2350265d55",
   "metadata": {},
   "source": [
    "### StreetEasy Mott Haven Buildings, Attempt 2\n",
    "\n",
    "Previously, we attempted to scrape the [list of buildings in Mott Haven](https://streeteasy.com/buildings/mott-haven) from the StreetEasy website through `BeautifulSoup`. However, the page seems to use JS to dynamically load content. So, we'll try scraping through headless browser `Playwright`.\n",
    "\n",
    "The details we need are the following:\n",
    "\n",
    "* building name\n",
    "* address\n",
    "* coordinates (for mapping)\n",
    "* year it was built\n",
    "* number of stories\n",
    "* number of units\n",
    "* link to individual pages (which we will use to get more details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407f78fb-3421-4035-8047-3c0f722e3b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "from playwright.async_api import async_playwright\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from random import randrange\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f5bb29c-eca3-456b-a300-38b0eca6ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snoozer\n",
    "\n",
    "def snoozer(start_time, end_time):\n",
    "    '''\n",
    "    This function creates a snoozer that can be used when scraping.\n",
    "    It requires `from random import randrange` and `import time`. \n",
    "    \n",
    "    Parameters: \n",
    "    start_time (int) = start time of range, in seconds\n",
    "    end_time (int) = end time of range, in seconds\n",
    "    '''\n",
    "    timer = randrange(start_time, end_time)\n",
    "    print(f\"Snoozing for {timer} seconds...\")\n",
    "    time.sleep(timer)\n",
    "    print(\"\") # adds a line break for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e4451-5ae5-4682-9e78-b1d3cc464348",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAIN CODE\n",
    "# this is tweaked from the bs4 scraper version\n",
    "\n",
    "base_url = \"https://streeteasy.com/buildings/mott-haven\"\n",
    "end_page = 2 # number of pages we want to scrape\n",
    "errors_list = [] # holds pages with errors\n",
    "main_df = [] # holds all captured lists\n",
    "\n",
    "# starting playwright\n",
    "async def scraper():\n",
    "    '''\n",
    "    This function scrapes a page asynchronously.\n",
    "    '''\n",
    "    async with async_playwright() as playwright:\n",
    "        browser = await playwright.firefox.launch(headless=False) # False because I want to see it load the page\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "    \n",
    "        try:\n",
    "            for page_num in range(1, end_page + 1):\n",
    "                print(f\"Attempting to scrape page {page_num}...\")\n",
    "                \n",
    "                ## requesting URLs\n",
    "                if page_num != 1:\n",
    "                    url = f\"{base_url}?page={page_num}\"\n",
    "                else:\n",
    "                    url = base_url\n",
    "                await page.goto(url)\n",
    "                await page.wait_for_load_state('networkidle')\n",
    "        \n",
    "                ## extracting data\n",
    "                # target_items = await page.query_selector_all(\"li.item.building\")\n",
    "            \n",
    "                # building_names = [ await target.query_selector(\"h2.details-title\").inner_text() for target in target_items ]\n",
    "                # building_links = [ \"https://streeteasy.com\" + await query_selector(\"a\").get_attribute(\"href\") for target in target_items]\n",
    "                # building_addresses = [ await target.query_selector(\"ul li\").inner_text() for target in target_items ]\n",
    "                # building_latlng = [ await target.get_attribute(\"se:map:point\") for target in target_items]\n",
    "            \n",
    "                # ## other data held separately\n",
    "                # for target in target_items:        \n",
    "                #     other_details = await target.query_selector(\"ul.details_info li.detail_cell\")        \n",
    "                #     building_units = [ await other_details[0].inner_text().replace(\" units\", \"\").strip() ]\n",
    "                #     building_stories = [ await other_details[1].inner_text().replace(\" stories\", \"\").strip() ]\n",
    "                #     building_year = [ await other_details[2].inner_text().replace(\"built in\", \"\").strip() ]\n",
    "    \n",
    "                print(f\"Successfuly scraped page {page_num}!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            errors_list.append(url)\n",
    "            print(f\"Error '{e}' was found on {url}, page {page_num} of {end_page} pages. Moving to next scrape...\")\n",
    "            \n",
    "        finally:\n",
    "            # ## create df to hold all data\n",
    "            # main_df.append(pd.DataFrame({ \"building_name\": building_names,\n",
    "                                         # \"link\": building_links,\n",
    "                                         # \"address\": building_addresses,\n",
    "                                         # \"coordinates\": building_latlng,\n",
    "                                         # \"year_built\": building_year,\n",
    "                                         # \"total_stories\": building_stories,\n",
    "                                         # \"total_units\": building_units\n",
    "                                        # }))\n",
    "            \n",
    "            snoozer(21, 56)\n",
    "    \n",
    "            # closing playwright\n",
    "            await browser.close()\n",
    "            print(f\"Done scraping {end_page} pages!\") \n",
    "\n",
    "nest_asyncio.apply()\n",
    "asyncio.run(scraper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e16896-53e6-41df-92d7-1eb60eff5583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
