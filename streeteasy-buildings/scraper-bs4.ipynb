{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8fc9163-55ca-4ee6-82f3-a456d87bc67c",
   "metadata": {},
   "source": [
    "### StreetEasy Mott Haven Buildings, Attempt 1\n",
    "\n",
    "In this notebook, we will be scraping the [list of buildings in Mott Haven](https://streeteasy.com/buildings/mott-haven) from the StreetEasy website. The details we need are the following:\n",
    "\n",
    "* building name\n",
    "* address\n",
    "* coordinates (for mapping)\n",
    "* year it was built\n",
    "* number of stories\n",
    "* number of units\n",
    "* link to individual pages (which we will use to get more details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d5e93a-ef6c-41f2-8244-2b3112210d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from random import randrange\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4ba8068-6639-4362-9153-bd44f0d75bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial\n",
    "\n",
    "url = \"https://streeteasy.com/buildings/mott-haven\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ad9a13-99a2-4438-9425-a3feb1880326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820c8479-e6fe-42a7-905e-e35c9a2893c2",
   "metadata": {},
   "source": [
    "### Excerpt source code that we wanted to target:\n",
    "\n",
    "```\n",
    "<li id=\"building_6257250\" class=\"item building\" data-id=\"6257250\" se:behavior='selectable hoverable clickable rememberable mappable' se:map='map'se:map:point='40.8078,-73.9291' data-blockindex='0'>\n",
    "  <div class=\"photo\" se:behavior='selectable hoverable clickable rememberable mappable' se:map='map'se:map:point='40.8078,-73.9291'  se:map='map'se:map:point='40.8078,-73.9291'>\n",
    "      <span id=\"saved_banner_6257250\"></span>\n",
    "    <a href=\"/building/the-crescendo\"><img alt=\"The Crescendo  at 25 Bruckner Boulevard in Mott Haven\" class=\"performance-marked\" data-performance-mark=\"search.Buildings.listingImageVisible\" src=\"https://photos.zillowstatic.com/fp/014287801b4057e345148514ce7e04da-se_medium_500_250.webp\" /></a>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"details row\">\n",
    "    <h2 class=\"details-title\">\n",
    "      <a se:clickable:target=\"true\" href=\"/building/the-crescendo\">The Crescendo </a>\n",
    "\n",
    "        <span id=\"buttons_6257250\"></span>\n",
    "\n",
    "            <div class=\"se_embed_react\" data-se-entry=\"userAuth\" data-se-component=\"UserAuthModal\" data-se-id=\"se_embed_react_eb19c5f5-880f-429b-b611-3ab94295952c\" data-react-component=\"\"></div>\n",
    "    <script>\n",
    "    window[\"se_embed_react_eb19c5f5-880f-429b-b611-3ab94295952c\"] = [\"UserAuthModal\",{}]\n",
    "    </script>\n",
    "\n",
    "    </h2>\n",
    "\n",
    "    <ul>\n",
    "        <li>At 25 Bruckner Boulevard</li>\n",
    "        <li class=\"price-info\">\n",
    "          <span class='price'>4 active rentals</span>\n",
    "        </li>\n",
    "\n",
    "        <li>\n",
    "          <ul class=\"details_info\"><li class=\"detail_cell\">130 units</li><li class=\"detail_cell\">6 stories</li><li class=\"detail_cell\">built in 2017</li></ul>\n",
    "        </li>\n",
    "        <li>\n",
    "          <div class=\"details_info\"><span class=\"detail_cell\">Rental Building in Mott Haven</span></div>\n",
    "        </li>\n",
    "        <li id=\"saved_section_6257250\">\n",
    "          <span class=\"u-visuallyHidden\">saved_section</span>\n",
    "        </li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</li>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230d34e7-b861-4290-8606-331a51d25dbc",
   "metadata": {},
   "source": [
    "## Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "527c0c7c-3b92-4efc-b9f2-aa781faa962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check status of pages\n",
    "\n",
    "def status_check(response):\n",
    "    '''\n",
    "    This function checks the status_code of a URL being requested.\n",
    "\n",
    "    Parameter:\n",
    "    response = requests.get(url)\n",
    "    '''\n",
    "    if 200 <= response.status_code <= 299:\n",
    "        print(\"Page is accessible. Scraping begins...\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}. We can't proceed.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eae4e415-021a-4fa4-92fc-0557c954ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snoozer\n",
    "\n",
    "def snoozer(start_time, end_time):\n",
    "    '''\n",
    "    This function creates a snoozer that can be used when scraping.\n",
    "    It requires `from random import randrange` and `import time`. \n",
    "    \n",
    "    Parameters: \n",
    "    start_time (int) = start time of range, in seconds\n",
    "    end_time (int) = end time of range, in seconds\n",
    "    '''\n",
    "    timer = randrange(start_time, end_time)\n",
    "    print(f\"Snoozing for {timer} seconds...\")\n",
    "    time.sleep(timer)\n",
    "    print(\"\") # adds a line break for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5f5d20b-3997-443e-aca6-f8c1e326b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers to get through that 403 error\n",
    "\n",
    "def get_session():\n",
    "    '''\n",
    "    This function creates a session with common headers.\n",
    "    '''\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\"\n",
    "    })\n",
    "    return session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7ea3e1-2191-4c61-8c2b-2f79dba7799f",
   "metadata": {},
   "source": [
    "### Let's try this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd114dd3-6e72-4ee8-be0e-195d003e7283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to scrape page 1 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 1!\n",
      "Snoozing for 107 seconds...\n",
      "\n",
      "Attempting to scrape page 2 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 2!\n",
      "Snoozing for 58 seconds...\n",
      "\n",
      "Attempting to scrape page 3 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 3!\n",
      "Snoozing for 77 seconds...\n",
      "\n",
      "Attempting to scrape page 4 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 4!\n",
      "Snoozing for 88 seconds...\n",
      "\n",
      "Attempting to scrape page 5 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 5!\n",
      "Snoozing for 122 seconds...\n",
      "\n",
      "Attempting to scrape page 6 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 6!\n",
      "Snoozing for 76 seconds...\n",
      "\n",
      "Attempting to scrape page 7 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 7!\n",
      "Snoozing for 125 seconds...\n",
      "\n",
      "Attempting to scrape page 8 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 8!\n",
      "Snoozing for 103 seconds...\n",
      "\n",
      "Attempting to scrape page 9 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 9!\n",
      "Snoozing for 97 seconds...\n",
      "\n",
      "Attempting to scrape page 10 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 10!\n",
      "Snoozing for 67 seconds...\n",
      "\n",
      "Attempting to scrape page 11 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 11!\n",
      "Snoozing for 119 seconds...\n",
      "\n",
      "Attempting to scrape page 12 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 12!\n",
      "Snoozing for 71 seconds...\n",
      "\n",
      "Attempting to scrape page 13 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 13!\n",
      "Snoozing for 115 seconds...\n",
      "\n",
      "Attempting to scrape page 14 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 14!\n",
      "Snoozing for 123 seconds...\n",
      "\n",
      "Attempting to scrape page 15 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 15!\n",
      "Snoozing for 105 seconds...\n",
      "\n",
      "Attempting to scrape page 16 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 16!\n",
      "Snoozing for 76 seconds...\n",
      "\n",
      "Attempting to scrape page 17 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 17!\n",
      "Snoozing for 96 seconds...\n",
      "\n",
      "Attempting to scrape page 18 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 18!\n",
      "Snoozing for 74 seconds...\n",
      "\n",
      "Attempting to scrape page 19 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 19!\n",
      "Snoozing for 125 seconds...\n",
      "\n",
      "Attempting to scrape page 20 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 20!\n",
      "Snoozing for 86 seconds...\n",
      "\n",
      "Attempting to scrape page 21 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 21!\n",
      "Snoozing for 97 seconds...\n",
      "\n",
      "Attempting to scrape page 22 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 22!\n",
      "Snoozing for 93 seconds...\n",
      "\n",
      "Attempting to scrape page 23 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 23!\n",
      "Snoozing for 116 seconds...\n",
      "\n",
      "Attempting to scrape page 24 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 24!\n",
      "Snoozing for 118 seconds...\n",
      "\n",
      "Attempting to scrape page 25 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 25!\n",
      "Snoozing for 103 seconds...\n",
      "\n",
      "Attempting to scrape page 26 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 26!\n",
      "Snoozing for 122 seconds...\n",
      "\n",
      "Attempting to scrape page 27 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 27!\n",
      "Snoozing for 97 seconds...\n",
      "\n",
      "Attempting to scrape page 28 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 28!\n",
      "Snoozing for 100 seconds...\n",
      "\n",
      "Attempting to scrape page 29 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 29!\n",
      "Snoozing for 54 seconds...\n",
      "\n",
      "Attempting to scrape page 30 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 30!\n",
      "Snoozing for 96 seconds...\n",
      "\n",
      "Attempting to scrape page 31 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 31!\n",
      "Snoozing for 124 seconds...\n",
      "\n",
      "Attempting to scrape page 32 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 32!\n",
      "Snoozing for 122 seconds...\n",
      "\n",
      "Attempting to scrape page 33 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 33!\n",
      "Snoozing for 113 seconds...\n",
      "\n",
      "Attempting to scrape page 34 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 34!\n",
      "Snoozing for 71 seconds...\n",
      "\n",
      "Attempting to scrape page 35 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 35!\n",
      "Snoozing for 70 seconds...\n",
      "\n",
      "Attempting to scrape page 36 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 36!\n",
      "Snoozing for 105 seconds...\n",
      "\n",
      "Attempting to scrape page 37 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 37!\n",
      "Snoozing for 131 seconds...\n",
      "\n",
      "Attempting to scrape page 38 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 38!\n",
      "Snoozing for 79 seconds...\n",
      "\n",
      "Attempting to scrape page 39 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 39!\n",
      "Snoozing for 124 seconds...\n",
      "\n",
      "Attempting to scrape page 40 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 40!\n",
      "Snoozing for 120 seconds...\n",
      "\n",
      "Attempting to scrape page 41 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 41!\n",
      "Snoozing for 111 seconds...\n",
      "\n",
      "Attempting to scrape page 42 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 42!\n",
      "Snoozing for 131 seconds...\n",
      "\n",
      "Attempting to scrape page 43 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 43!\n",
      "Snoozing for 85 seconds...\n",
      "\n",
      "Attempting to scrape page 44 of 45 pages...\n",
      "Items found on this page: 11\n",
      "Successfuly scraped page 44!\n",
      "Snoozing for 99 seconds...\n",
      "\n",
      "Attempting to scrape page 45 of 45 pages...\n",
      "Items found on this page: 4\n",
      "Successfuly scraped page 45!\n"
     ]
    }
   ],
   "source": [
    "# initializing variables\n",
    "base_url = \"https://streeteasy.com/buildings/mott-haven\"\n",
    "end_page = 45 # changed to 45 when ready to scrape\n",
    "errors_list = [] # holds pages with errors\n",
    "all_data = [] # holds all captured data\n",
    "\n",
    "session = get_session()\n",
    "\n",
    "# initializing data dictionary\n",
    "data_dict = {\n",
    "    \"building_name\": [],\n",
    "    \"link\": [],\n",
    "    \"address\": [],\n",
    "    \"coordinates\": [],\n",
    "    \"total_units\": [],\n",
    "    \"total_stories\": [],\n",
    "    \"year_built\": []\n",
    "}\n",
    "\n",
    "for page_num in range(1, end_page + 1):     \n",
    "    print(f\"Attempting to scrape page {page_num} of {end_page} pages...\")\n",
    "    \n",
    "    try:\n",
    "        # request URL\n",
    "        if page_num != 1:\n",
    "            url = f\"{base_url}?page={page_num}\"\n",
    "        else:\n",
    "            url = base_url\n",
    "        \n",
    "        # retry mechanisms\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = session.get(url, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                break\n",
    "            except requests.RequestException as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise e\n",
    "                snoozer(10, 15)  # longer wait between retries\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # extracting data\n",
    "        target_items = soup.find_all(\"li\", class_=\"item building\")\n",
    "        print(f\"Items found on this page: {len(target_items)}\")\n",
    "\n",
    "        # in case of errors\n",
    "        if not target_items:\n",
    "            print(f\"Warning: No building data found on page {page_num}.\")\n",
    "            errors_list.append(url)\n",
    "            continue\n",
    "\n",
    "        for target in target_items:\n",
    "            data_dict[\"building_name\"].append(target.find(\"h2\", class_=\"details-title\").get_text(strip=True).replace(\"SAVE\", \"\")\\\n",
    "                                  if target.find(\"h2\", class_=\"details-title\") else np.nan)\n",
    "            \n",
    "            data_dict[\"link\"].append(\"https://streeteasy.com\" + target.find(\"a\").get(\"href\")\\\n",
    "                             if target.find(\"a\").get(\"href\") else np.nan)\n",
    "            \n",
    "            data_dict[\"coordinates\"].append(target.get(\"se:map:point\")\\\n",
    "                              if target.get(\"se:map:point\") else np.nan)\n",
    "    \n",
    "            # other data held separately\n",
    "            address_tag = target.find(\"ul\").find(\"li\") if target.find(\"ul\") else None\n",
    "            address = (address_tag.get_text(strip=True).replace(\"At \", \"\")\\\n",
    "                       if address_tag and \"At \" in address_tag.get_text() else np.nan)\n",
    "            data_dict[\"address\"].append(address)\n",
    "\n",
    "            # initialize variables\n",
    "            units = stories = year = np.nan\n",
    "    \n",
    "            other_details = target.find(\"ul\", class_=\"details_info\")\n",
    "            if other_details: \n",
    "                for detail in other_details.find_all(\"li\", class_=\"detail_cell\"):\n",
    "                    text = detail.get_text(strip=True)\n",
    "                    try:\n",
    "                        if \"units\" in text:\n",
    "                            units = int(text.split()[0])\n",
    "                        elif \"stories\" in text:\n",
    "                            stories = int(text.split()[0])\n",
    "                        elif \"built in\" in text:\n",
    "                            year = int(text.split()[-1])\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            data_dict[\"total_units\"].append(units)\n",
    "            data_dict[\"total_stories\"].append(stories)\n",
    "            data_dict[\"year_built\"].append(year)\n",
    "        \n",
    "        print(f\"Successfuly scraped page {page_num}!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        errors_list.append(url)\n",
    "        print(f\"Error '{e}' was found on {url}, page {page_num} of {end_page} pages. Moving to next scrape...\")\n",
    "        continue\n",
    "    \n",
    "    finally:        \n",
    "        if page_num <= end_page - 1:\n",
    "            snoozer(53, 132)\n",
    "\n",
    "# saving to df\n",
    "df = pd.DataFrame(data_dict)\n",
    "all_data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "112bf216-1f6b-4f18-a1e0-2267d9d7ea7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "488"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting to our final df\n",
    "\n",
    "final_df = pd.concat(all_data, ignore_index=True)\n",
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c086ef64-8187-4b92-a556-9ee43df1065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the file to csv\n",
    "final_df.to_csv(\"mott-haven-streeteasy-buildings.csv\", encoding=\"UTF-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4bad1-0fc1-497b-a68a-19182f8c8e87",
   "metadata": {},
   "source": [
    "## Failed attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3aa283-cd6c-4b87-ab19-eec5eeba5175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## MAIN CODE, DON'T RUN \n",
    "\n",
    "# base_url = \"https://streeteasy.com/buildings/mott-haven\"\n",
    "# end_page = 45 # number of pages we want to scrape\n",
    "# errors_list = [] # holds pages with errors\n",
    "# main_df = [] # holds all captured lists\n",
    "\n",
    "# for page_num in range(1, end_page + 1):\n",
    "#     print(f\"Page {page_num}:\")\n",
    "    \n",
    "#     ## requesting URLs\n",
    "#     if page_num != 1: # not the first page\n",
    "#         response = requests.get(f\"{base_url}?page={page_num}\")\n",
    "#     else: # this is the first page\n",
    "#         response = requests.get(base_url)\n",
    "#     if not status_check(response):\n",
    "#         errors_list.append(f\"{base_url}?page={page_num}\")\n",
    "        \n",
    "#     ## soupifying the response\n",
    "#     soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "#     # ## extracting data\n",
    "#     # target_items = soup.find_all(\"li\", class_=\"item building\")\n",
    "\n",
    "#     # building_names = [ target.find(\"h2\", class_=\"details-title\").get_text(strip=True) for target in target_items ]\n",
    "#     # building_links = [ \"https://streeteasy.com\" + target.find(\"a\").get(\"href\") for target in target_items]\n",
    "#     # building_addresses = [ target.find(\"ul\").find(\"li\").get_text(strip=True) for target in target_items ]\n",
    "#     # building_latlng = [ target.get(\"se:map:point\") for target in target_items]\n",
    "\n",
    "#     # ## other data held separately\n",
    "\n",
    "#     # for target in target_items:        \n",
    "#     #     other_details = target.find(\"ul\", class_=\"details_info\").find_all(\"li\", class_=\"detail_cell\")        \n",
    "#     #     building_units = [ other_details[0].replace(\" units\", \"\") ]\n",
    "#     #     building_stories = [ other_details[1].replace(\" stories\", \"\") ]\n",
    "#     #     building_year = [ other_details[2].replace(\"built in\", \"\") ]\n",
    "\n",
    "#     # ## create df to hold all data\n",
    "\n",
    "#     # main_df.append(pd.DataFrame({ \"building_name\": building_names,\n",
    "#     #                              \"link\": building_links,\n",
    "#     #                              \"address\": building_addresses,\n",
    "#     #                              \"coordinates\": building_latlng,\n",
    "#     #                              \"year_built\": building_year,\n",
    "#     #                              \"total_stories\": building_stories,\n",
    "#     #                              \"total_units\": building_units\n",
    "#     #                             }))\n",
    "\n",
    "#     snoozer(21, 56)\n",
    "        \n",
    "# print(f\"Done scraping {end_page} pages!\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644c2db-f55b-4d6b-97cd-c4e96c0654f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
